import os
os.environ["LLAMA_CPP_LIB"] = "C:\\bin\\dev\\llama\\llama.cpp\\build\\bin\\Release\\llama.dll"

os.add_dll_directory("C:\\bin\\dev\\CUDA\\bin")
os.add_dll_directory("C:\\bin\\dev\\llama\\llama.cpp\\build\\bin\\Release")

import ctypes
ctypes.WinDLL("C:\\bin\\dev\\llama\\llama.cpp\\build\\bin\\Release\\ggml-cuda.dll")
ctypes.WinDLL("C:\\bin\\dev\\llama\\llama.cpp\\build\\bin\\Release\\llama.dll")

import llama_cpp
print("Verwendete llama_cpp-Python-Bindings:", llama_cpp.__file__)
print("LLAMA_CPP_LIB:", os.environ.get("LLAMA_CPP_LIB"))

from llama_cpp import Llama

llm = Llama(
    model_path="C:\\usr\\dev\\llmodels\\mistral-7b-openorca.Q4_K_M.gguf",
    n_ctx=4096,
    n_gpu_layers=-1,
    n_threads=6,
    use_mlock=True,
    verbose=True
)

antwort = llm("Was ist der Sinn des Lebens?\n", max_tokens=128)
print(antwort["choices"][0]["text"])